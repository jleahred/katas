# Elixir Axon tutorial

```elixir
Mix.install([
  {:nx, "~> 0.2.0"},
  {:exla, "~> 0.2.0"},
  {:axon, "~> 0.1.0-dev", github: "elixir-nx/axon", branch: "main"}
])
```

<!-- livebook:{"output":true} -->

```
:ok
```

## Aim

We will create and train a neural network to detect oddity on one byte numbers

## Create a neural network

Let's start creating a neural network for our aim

```elixir
require Axon

model =
  Axon.input({nil, 8})
  |> Axon.dense(20, activation: :relu)
  |> Axon.dense(20, activation: :relu)
  # |> Axon.dropout(rate: 0.5)
  |> Axon.dense(2, activation: :softmax)
```

<!-- livebook:{"output":true} -->

```
---------------------------------------------------------------------------------------------------
                                               Model
===================================================================================================
 Layer                              Shape       Policy              Parameters   Parameters Memory
===================================================================================================
 input_0 ( input )                  {nil, 8}    p=f32 c=f32 o=f32   0            0 bytes
 dense_0 ( dense["input_0"] )       {nil, 20}   p=f32 c=f32 o=f32   180          720 bytes
 relu_0 ( relu["dense_0"] )         {nil, 20}   p=f32 c=f32 o=f32   0            0 bytes
 dense_1 ( dense["relu_0"] )        {nil, 20}   p=f32 c=f32 o=f32   420          1680 bytes
 relu_1 ( relu["dense_1"] )         {nil, 20}   p=f32 c=f32 o=f32   0            0 bytes
 dense_2 ( dense["relu_1"] )        {nil, 2}    p=f32 c=f32 o=f32   42           168 bytes
 softmax_0 ( softmax["dense_2"] )   {nil, 2}    p=f32 c=f32 o=f32   0            0 bytes
---------------------------------------------------------------------------------------------------

```

Easy-peasy

As you can see, its a network with 8 neurons as input

<!-- livebook:{"force_markdown":true} -->

```elixir
  Axon.input({nil, 8})
```

Later, we put two small layers of neurons (just 20 neurons per layer, it looks an easy problem and probably is not necessary more)

<!-- livebook:{"force_markdown":true} -->

```elixir
  |> Axon.dense(20, activation: :relu)
  |> Axon.dense(20, activation: :relu)
```

And to finish, just two neurons as output

<!-- livebook:{"force_markdown":true} -->

```elixir
  |> Axon.dense(2, activation: :softmax)
```

And... that's all we already have our **Neuron network**

With a little initialization, we already can ask to our network

Let's do it!

```elixir
state_init = model |> Axon.init(compiler: EXLA)
```

<!-- livebook:{"output":true} -->

```
%{
  "dense_0" => %{
    "bias" => #Nx.Tensor<
      f32[20]
      EXLA.Backend<host:0, 0.3092930912.1915093000.72356>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
    >,
    "kernel" => #Nx.Tensor<
      f32[8][20]
      EXLA.Backend<host:0, 0.3092930912.1915093000.72357>
      [
        [-0.10607001185417175, -0.30189943313598633, -0.40362775325775146, -0.4041273891925812, -0.30879074335098267, 0.20781314373016357, -0.20583578944206238, -0.15819194912910461, 0.2368161678314209, 0.4220193028450012, 0.23583722114562988, 0.1358742117881775, 0.18023717403411865, -0.291803240776062, 0.37833887338638306, -0.2605515718460083, 0.26422399282455444, -0.34602999687194824, 0.24145424365997314, 0.22324973344802856],
        [0.06343042850494385, -0.23676276206970215, -0.02762901782989502, -0.4595149755477905, -0.015001446008682251, -0.3792414367198944, 0.19602245092391968, 0.36634886264801025, 0.3947668671607971, -0.41211459040641785, -0.36001449823379517, -0.26531144976615906, 0.20114368200302124, 0.39050567150115967, -0.3010143041610718, 0.1392688751220703, 0.10187220573425293, 0.3182387351989746, 0.349526584148407, 0.24164259433746338],
        [0.36727166175842285, 0.37648725509643555, 0.3341730833053589, 0.0646665096282959, 0.3113142251968384, 0.45901215076446533, 0.26793819665908813, ...],
        ...
      ]
    >
  },
  "dense_1" => %{
    "bias" => #Nx.Tensor<
      f32[20]
      EXLA.Backend<host:0, 0.3092930912.1915093000.72358>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
    >,
    "kernel" => #Nx.Tensor<
      f32[20][20]
      EXLA.Backend<host:0, 0.3092930912.1915093000.72359>
      [
        [-0.015194118022918701, -0.24075311422348022, 0.27031999826431274, -0.3441360890865326, 0.267170786857605, 0.2444465160369873, -0.03655880689620972, -0.1900617927312851, 0.14298224449157715, 0.27850496768951416, -0.22707833349704742, 0.3351489305496216, 0.05984276533126831, -0.14882665872573853, -0.011875182390213013, 0.1757398247718811, -0.1675986498594284, -0.28917962312698364, -0.026128381490707397, -0.12328499555587769],
        [0.09326103329658508, -0.07958570122718811, 0.33431416749954224, -0.032362014055252075, -0.12680450081825256, -0.1596331000328064, -0.01247096061706543, -0.3578503727912903, -0.08661872148513794, -0.31791824102401733, 0.30378109216690063, 0.16949057579040527, 0.1980125904083252, -0.09483754634857178, -0.21890974044799805, 0.350757896900177, 0.24140888452529907, 0.05424922704696655, -0.10002017021179199, 0.009107589721679688],
        [0.02156689763069153, -0.32964521646499634, 0.38603734970092773, 0.3177887797355652, -0.10048139095306396, -0.22337840497493744, ...],
        ...
      ]
    >
  },
  "dense_2" => %{
    "bias" => #Nx.Tensor<
      f32[2]
      EXLA.Backend<host:0, 0.3092930912.1915093000.72360>
      [0.0, 0.0]
    >,
    "kernel" => #Nx.Tensor<
      f32[20][2]
      EXLA.Backend<host:0, 0.3092930912.1915093000.72361>
      [
        [-0.3323746919631958, 0.22447752952575684],
        [0.04987424612045288, -0.09851944446563721],
        [0.03532135486602783, -0.3011925220489502],
        [0.18714570999145508, -0.26838961243629456],
        [-0.46550169587135315, -0.41150331497192383],
        [0.3569929599761963, 0.2299444079399109],
        [-0.5058806538581848, -0.2972293496131897],
        [-0.3276469111442566, 0.23066407442092896],
        [0.39945822954177856, -0.20604196190834045],
        [-0.023125380277633667, 0.3421294093132019],
        [0.0603603720664978, 0.2278650999069214],
        [0.3414663076400757, 0.3048752546310425],
        [0.04513365030288696, 0.13778293132781982],
        [0.5131054520606995, 0.4461209774017334],
        [-0.21521297097206116, 0.4683728814125061],
        [-0.06630980968475342, -0.005020499229431152],
        [-0.15720686316490173, 0.1496146321296692],
        [-0.44577956199645996, 0.16049665212631226],
        [-0.05451783537864685, -0.12924665212631226],
        [-0.4368118345737457, -0.16920316219329834]
      ]
    >
  }
}
```

Now we have a **neural network** initialized and we can ask to it

Let's start asking for number 0

```elixir
Axon.predict(
  model,
  state_init,
  %{
    "input_0" => Nx.tensor([[0.0, 0.0, 0, 0, 0, 0, 0.0, 0.0]])
  },
  compiler: EXLA
)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[1][2]
  EXLA.Backend<host:0, 0.3092930912.1915093000.72363>
  [
    [0.5, 0.5]
  ]
>
```

The values of the tensor should say as the probability of be odd or even

> **[0.5, 0.5]**

As you can see, the system has no clue about the answer

And with number 1, similar result

It's quite logical, at the moment we only have a neural network with a specific topology but we didn't add to the network any information about odd and even numbers

How to?

**Trainning**

## Prepare data to train

First we need data to train

It consist on input values, with the right response

Let's start with something simple

```elixir
train_data = [
  {Nx.tensor([[0.0, 0, 0, 0, 0, 0, 0, 1.0]]), Nx.tensor([0.0, 1.0])},
  {Nx.tensor([[0.0, 0, 0, 0, 0, 0, 0.0, 0.0]]), Nx.tensor([1.0, 0.0])}
]
```

<!-- livebook:{"output":true} -->

```
[
  {#Nx.Tensor<
     f32[1][8]
     [
       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]
     ]
   >,
   #Nx.Tensor<
     f32[2]
     [0.0, 1.0]
   >},
  {#Nx.Tensor<
     f32[1][8]
     [
       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
     ]
   >,
   #Nx.Tensor<
     f32[2]
     [1.0, 0.0]
   >}
]
```

And update the neural network with these training data

```elixir
trained0 =
  model
  |> Axon.Loop.trainer(:categorical_cross_entropy, Axon.Optimizers.adamw(0.005))
  |> Axon.Loop.run(train_data, epochs: 100, compiler: EXLA)
```

<!-- livebook:{"output":true} -->

```
Epoch: 0, Batch: 0, loss: 0.0000000
Epoch: 1, Batch: 0, loss: 0.6687524
Epoch: 2, Batch: 0, loss: 0.6484462
Epoch: 3, Batch: 0, loss: 0.6326350
Epoch: 4, Batch: 0, loss: 0.6192583
Epoch: 5, Batch: 0, loss: 0.6069476
Epoch: 6, Batch: 0, loss: 0.5952771
Epoch: 7, Batch: 0, loss: 0.5840591
Epoch: 8, Batch: 0, loss: 0.5731463
Epoch: 9, Batch: 0, loss: 0.5627925
Epoch: 10, Batch: 0, loss: 0.5526837
Epoch: 11, Batch: 0, loss: 0.5427136
Epoch: 12, Batch: 0, loss: 0.5328447
Epoch: 13, Batch: 0, loss: 0.5231585
Epoch: 14, Batch: 0, loss: 0.5136282
Epoch: 15, Batch: 0, loss: 0.5042270
Epoch: 16, Batch: 0, loss: 0.4949725
Epoch: 17, Batch: 0, loss: 0.4858423
Epoch: 18, Batch: 0, loss: 0.4768410
Epoch: 19, Batch: 0, loss: 0.4679712
Epoch: 20, Batch: 0, loss: 0.4591900
Epoch: 21, Batch: 0, loss: 0.4505542
Epoch: 22, Batch: 0, loss: 0.4420328
Epoch: 23, Batch: 0, loss: 0.4336167
Epoch: 24, Batch: 0, loss: 0.4253021
Epoch: 25, Batch: 0, loss: 0.4170941
Epoch: 26, Batch: 0, loss: 0.4089954
Epoch: 27, Batch: 0, loss: 0.4010240
Epoch: 28, Batch: 0, loss: 0.3931581
Epoch: 29, Batch: 0, loss: 0.3853932
Epoch: 30, Batch: 0, loss: 0.3777454
Epoch: 31, Batch: 0, loss: 0.3702232
Epoch: 32, Batch: 0, loss: 0.3628266
Epoch: 33, Batch: 0, loss: 0.3555545
Epoch: 34, Batch: 0, loss: 0.3484132
Epoch: 35, Batch: 0, loss: 0.3414054
Epoch: 36, Batch: 0, loss: 0.3345423
Epoch: 37, Batch: 0, loss: 0.3278255
Epoch: 38, Batch: 0, loss: 0.3212553
Epoch: 39, Batch: 0, loss: 0.3148389
Epoch: 40, Batch: 0, loss: 0.3085791
Epoch: 41, Batch: 0, loss: 0.3024820
Epoch: 42, Batch: 0, loss: 0.2965444
Epoch: 43, Batch: 0, loss: 0.2907692
Epoch: 44, Batch: 0, loss: 0.2851566
Epoch: 45, Batch: 0, loss: 0.2797055
Epoch: 46, Batch: 0, loss: 0.2744155
Epoch: 47, Batch: 0, loss: 0.2692818
Epoch: 48, Batch: 0, loss: 0.2643023
Epoch: 49, Batch: 0, loss: 0.2594742
Epoch: 50, Batch: 0, loss: 0.2547938
Epoch: 51, Batch: 0, loss: 0.2502573
Epoch: 52, Batch: 0, loss: 0.2458602
Epoch: 53, Batch: 0, loss: 0.2415983
Epoch: 54, Batch: 0, loss: 0.2374673
Epoch: 55, Batch: 0, loss: 0.2334627
Epoch: 56, Batch: 0, loss: 0.2295799
Epoch: 57, Batch: 0, loss: 0.2258151
Epoch: 58, Batch: 0, loss: 0.2221634
Epoch: 59, Batch: 0, loss: 0.2186209
Epoch: 60, Batch: 0, loss: 0.2151831
Epoch: 61, Batch: 0, loss: 0.2118463
Epoch: 62, Batch: 0, loss: 0.2086067
Epoch: 63, Batch: 0, loss: 0.2054604
Epoch: 64, Batch: 0, loss: 0.2024038
Epoch: 65, Batch: 0, loss: 0.1994335
Epoch: 66, Batch: 0, loss: 0.1965460
Epoch: 67, Batch: 0, loss: 0.1937385
Epoch: 68, Batch: 0, loss: 0.1910077
Epoch: 69, Batch: 0, loss: 0.1883508
Epoch: 70, Batch: 0, loss: 0.1857650
Epoch: 71, Batch: 0, loss: 0.1832475
Epoch: 72, Batch: 0, loss: 0.1807959
Epoch: 73, Batch: 0, loss: 0.1784077
Epoch: 74, Batch: 0, loss: 0.1760805
Epoch: 75, Batch: 0, loss: 0.1738122
Epoch: 76, Batch: 0, loss: 0.1716006
Epoch: 77, Batch: 0, loss: 0.1694436
Epoch: 78, Batch: 0, loss: 0.1673393
Epoch: 79, Batch: 0, loss: 0.1652860
Epoch: 80, Batch: 0, loss: 0.1632817
Epoch: 81, Batch: 0, loss: 0.1613248
Epoch: 82, Batch: 0, loss: 0.1594137
Epoch: 83, Batch: 0, loss: 0.1575468
Epoch: 84, Batch: 0, loss: 0.1557226
Epoch: 85, Batch: 0, loss: 0.1539398
Epoch: 86, Batch: 0, loss: 0.1521969
Epoch: 87, Batch: 0, loss: 0.1504926
Epoch: 88, Batch: 0, loss: 0.1488258
Epoch: 89, Batch: 0, loss: 0.1471951
Epoch: 90, Batch: 0, loss: 0.1455995
Epoch: 91, Batch: 0, loss: 0.1440378
Epoch: 92, Batch: 0, loss: 0.1425090
Epoch: 93, Batch: 0, loss: 0.1410121
Epoch: 94, Batch: 0, loss: 0.1395461
Epoch: 95, Batch: 0, loss: 0.1381100
Epoch: 96, Batch: 0, loss: 0.1367030
Epoch: 97, Batch: 0, loss: 0.1353242
Epoch: 98, Batch: 0, loss: 0.1339727
Epoch: 99, Batch: 0, loss: 0.1326478
```

<!-- livebook:{"output":true} -->

```
%{
  "dense_0" => %{
    "bias" => #Nx.Tensor<
      f32[20]
      EXLA.Backend<host:0, 0.3092930912.1915093000.77565>
      [-1.4242564793676138e-4, 0.0, 0.20980511605739594, -1.0443091014167294e-5, 5.767212132923305e-5, 0.0, -1.6108706768136472e-4, -2.325045206816867e-4, -4.3457065476104617e-4, 0.0, -1.574035850353539e-4, -5.383925163187087e-4, 0.0, 1.1155833635712042e-4, 0.0, 0.27045372128486633, -0.03002728708088398, 0.0, 0.5358567833900452, -0.052301034331321716]
    >,
    "kernel" => #Nx.Tensor<
      f32[8][20]
      EXLA.Backend<host:0, 0.3092930912.1915093000.77566>
      [
        [-0.1062420904636383, 0.37944382429122925, 0.026344656944274902, -0.27295631170272827, -0.20153194665908813, 0.29704225063323975, 0.39771896600723267, -0.338668555021286, -0.06709259748458862, -0.29232871532440186, 0.09678918123245239, -0.14682742953300476, 0.07515907287597656, -0.10501843690872192, -0.12385168671607971, 0.3673705458641052, -0.3996991515159607, 0.26049089431762695, 0.3805611729621887, 0.03409934043884277],
        [0.4086109399795532, -0.44592103362083435, 0.0443078875541687, -0.21373902261257172, -0.33495935797691345, 0.1575911045074463, 0.061364710330963135, 0.06126809120178223, -0.12664031982421875, 0.342568576335907, 0.13688433170318604, 0.4309094548225403, 0.019929945468902588, -0.32621514797210693, 0.19643211364746094, -0.18094509840011597, 0.03032281994819641, -0.45175012946128845, 0.03896075487136841, 0.3381856083869934],
        [0.4320094585418701, 0.42730122804641724, 0.052777647972106934, -0.34847405552864075, 0.41423505544662476, 0.030018776655197144, -0.2307009994983673, ...],
        ...
      ]
    >
  },
  "dense_1" => %{
    "bias" => #Nx.Tensor<
      f32[20]
      EXLA.Backend<host:0, 0.3092930912.1915093000.77567>
      [-0.10665438324213028, 0.32946857810020447, -0.03154921904206276, 0.0, -0.042286183685064316, -0.027794228866696358, -0.04223620891571045, -0.007541564293205738, 0.0, -0.15762385725975037, 0.3852362036705017, 0.0, -0.22127673029899597, 0.2509159445762634, -0.06492994725704193, -0.02565121464431286, 0.4375194311141968, 0.293430894613266, 0.0, -0.14146193861961365]
    >,
    "kernel" => #Nx.Tensor<
      f32[20][20]
      EXLA.Backend<host:0, 0.3092930912.1915093000.77568>
      [
        [0.41999757289886475, -0.3346937894821167, 0.1426410675048828, 0.2805417776107788, -0.31844377517700195, -0.021014459431171417, -0.028133446350693703, 0.07123257219791412, 0.004765152931213379, 0.057788144797086716, -0.6330597400665283, 0.3554607629776001, 0.6311308145523071, -0.1169271171092987, 0.2899933159351349, 0.6038464307785034, -0.37770622968673706, -0.22989238798618317, -0.029863685369491577, 0.3985556662082672],
        [-0.2627788484096527, 0.31841254234313965, 0.37474292516708374, 0.2812313437461853, -0.31782209873199463, 0.19877564907073975, 0.05367320775985718, 0.34616512060165405, 0.13424980640411377, -0.30106016993522644, -0.21855349838733673, -0.047065526247024536, 0.19067400693893433, 0.15170544385910034, -0.17020177841186523, -0.20371802151203156, 0.18894636631011963, 0.3339000344276428, -0.08786705136299133, 0.09037831425666809],
        [0.12406100332736969, 0.1780601590871811, -0.2797767221927643, -0.33087801933288574, 0.28744497895240784, -0.17230254411697388, ...],
        ...
      ]
    >
  },
  "dense_2" => %{
    "bias" => #Nx.Tensor<
      f32[2]
      EXLA.Backend<host:0, 0.3092930912.1915093000.77569>
      [0.14617978036403656, -0.14617981016635895]
    >,
    "kernel" => #Nx.Tensor<
      f32[20][2]
      EXLA.Backend<host:0, 0.3092930912.1915093000.77570>
      [
        [-0.370612770318985, 0.6864516139030457],
        [0.9878939986228943, -0.21638208627700806],
        [0.1056370958685875, 0.06607721000909805],
        [-0.48263153433799744, -0.07889989018440247],
        [0.2712278366088867, -0.16295269131660461],
        [0.20005884766578674, 0.26042720675468445],
        [0.48244157433509827, 0.20831996202468872],
        [-0.4088813364505768, 0.3574115037918091],
        [-0.14146432280540466, 0.4557270407676697],
        [-0.7071971893310547, 0.26929140090942383],
        [0.12146981060504913, -0.8678855895996094],
        [0.3477380871772766, 0.018942058086395264],
        [0.21558649837970734, 0.6854603886604309],
        [0.40945589542388916, -0.5708649754524231],
        [-0.41275671124458313, -0.41790103912353516],
        [-0.4255504310131073, 0.1866253912448883],
        [0.33812057971954346, -0.6558142304420471],
        [0.7590045928955078, -0.8905529379844666],
        [0.040322840213775635, 0.2025628685951233],
        [-0.5622349977493286, 0.28932616114616394]
      ]
    >
  }
}
```

Now we have a neural network **trainned** with a little information (but we repeated it a lot).

Lets ask to our NN again

```elixir
Axon.predict(
  model,
  trained0,
  %{
    "input_0" => Nx.tensor([[0.0, 0.0, 0, 0, 0, 0, 0.0, 0.0]])
  },
  compiler: EXLA
)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[1][2]
  EXLA.Backend<host:0, 0.3092930912.1915093000.77588>
  [
    [0.9957674741744995, 0.004232511855661869]
  ]
>
```

```elixir
Axon.predict(
  model,
  trained0,
  %{
    "input_0" => Nx.tensor([[0.0, 0.0, 0, 0, 0, 0, 0.0, 1.0]])
  },
  compiler: EXLA
)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[1][2]
  EXLA.Backend<host:0, 0.3092930912.1915093000.77590>
  [
    [0.001158886356279254, 0.9988411068916321]
  ]
>
```

That looks quite different!!!

But, what if we ask a not trainned number?

```elixir
Axon.predict(
  model,
  trained0,
  %{
    "input_0" => Nx.tensor([[0, 0, 0, 1, 0, 0, 0, 0]])
  },
  compiler: EXLA
)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[1][2]
  EXLA.Backend<host:0, 0.3092930912.1915093000.77592>
  [
    [0.9417206048965454, 0.05827938765287399]
  ]
>
```

Not very good answer

We have to train with several numbers, let's do it!

To do that, we want a big (infinity will be enough ;-) list of random numbers with the right response.

```elixir
random = Enum.random(0..255)
even = if rem(random, 2) == 0, do: true, else: false
{random, even}
```

<!-- livebook:{"output":true} -->

```
{62, true}
```

Let's put this in an infinite list

```elixir
raw_trainning_data =
  Stream.unfold(0, fn _ ->
    random = Enum.random(0..255)
    even = if rem(random, 2) == 0, do: true, else: false
    {{random, even}, 0}
  end)
```

<!-- livebook:{"output":true} -->

```
#Function<61.58486609/2 in Stream.unfold/2>
```

Show some data to verify

```elixir
raw_trainning_data |> Enum.take(10)
```

<!-- livebook:{"output":true} -->

```
[
  {26, true},
  {254, true},
  {242, true},
  {179, false},
  {166, true},
  {225, false},
  {202, true},
  {36, true},
  {61, false},
  {103, false}
]
```

Looks great, but... remember how we introduced our data in the network

<!-- livebook:{"force_markdown":true} -->

```elixir
train_data = [
  {Nx.tensor([[0.0, 0, 0, 0, 0, 0, 0, 1.0]]), Nx.tensor([0.0, 1.0])},
  {Nx.tensor([[0.0, 0, 0, 0, 0, 0, 0.0, 0.0]]), Nx.tensor([1.0, 0.0])}
]
```

Then we have to map to this format

First, a function to convert a number, to a tensor with the neurons to be activated

```elixir
number2input_tensor = fn num ->
  :io_lib.format("~8.2.0B", [num])
  |> Enum.map(fn d -> if d == ?0, do: 0.0, else: 1.0 end)
  |> Nx.tensor()
  |> Nx.reshape({1, 8})
end

number2input_tensor.(123)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[1][8]
  [
    [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]
  ]
>
```

And now, a function to convert true or false to the tensor result

```elixir
even2output_tensor = fn is_even ->
  if(is_even, do: Nx.tensor([1.0, 0.0]), else: Nx.tensor([0.0, 1.0]))
end

even2output_tensor.(false)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[2]
  [0.0, 1.0]
>
```

Remember we started...

```elixir
raw_trainning_data |> Enum.take(10)
```

<!-- livebook:{"output":true} -->

```
[
  {9, false},
  {139, false},
  {60, true},
  {18, true},
  {32, true},
  {182, true},
  {121, false},
  {248, true},
  {213, false},
  {7, false}
]
```

```elixir
trainning_data =
  raw_trainning_data
  |> Stream.map(fn {n, is_even} ->
    {number2input_tensor.(n), even2output_tensor.(is_even)}
  end)

trainning_data |> Enum.take(10)
```

<!-- livebook:{"output":true} -->

```
[
  {#Nx.Tensor<
     f32[1][8]
     [
       [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]
     ]
   >,
   #Nx.Tensor<
     f32[2]
     [1.0, 0.0]
   >},
  {#Nx.Tensor<
     f32[1][8]
     [
       [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]
     ]
   >,
   #Nx.Tensor<
     f32[2]
     [0.0, 1.0]
   >},
  {#Nx.Tensor<
     f32[1][8]
     [
       [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]
     ]
   >,
   #Nx.Tensor<
     f32[2]
     [0.0, 1.0]
   >},
  {#Nx.Tensor<
     f32[1][8]
     [
       [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]
     ]
   >,
   #Nx.Tensor<
     f32[2]
     [0.0, 1.0]
   >},
  {#Nx.Tensor<
     f32[1][8]
     [
       [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]
     ]
   >,
   #Nx.Tensor<
     f32[2]
     [0.0, 1.0]
   >},
  {#Nx.Tensor<
     f32[1][8]
     [
       [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]
     ]
   >,
   #Nx.Tensor<
     f32[2]
     [1.0, 0.0]
   >},
  {#Nx.Tensor<
     f32[1][8]
     [
       [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]
     ]
   >,
   #Nx.Tensor<
     f32[2]
     [0.0, 1.0]
   >},
  {#Nx.Tensor<
     f32[1][8]
     [
       [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]
     ]
   >,
   #Nx.Tensor<
     f32[2]
     [0.0, 1.0]
   >},
  {#Nx.Tensor<
     f32[1][8]
     [
       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]
     ]
   >,
   #Nx.Tensor<
     f32[2]
     [0.0, 1.0]
   >},
  {#Nx.Tensor<
     f32[1][8]
     [
       [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]
     ]
   >,
   #Nx.Tensor<
     f32[2]
     [1.0, 0.0]
   >}
]
```

## Trainning day!

Now we have a infinite list of data to train!

Just train it again with some datas

```elixir
trained1 =
  model
  |> Axon.Loop.trainer(:categorical_cross_entropy, Axon.Optimizers.adamw(0.005))
  |> Axon.Loop.run(trainning_data |> Stream.take(1000), epochs: 1, compiler: EXLA)
```

<!-- livebook:{"output":true} -->

```
Epoch: 0, Batch: 950, loss: 0.0582825
```

<!-- livebook:{"output":true} -->

```
%{
  "dense_0" => %{
    "bias" => #Nx.Tensor<
      f32[20]
      EXLA.Backend<host:0, 0.3092930912.1915093000.103594>
      [0.19677899777889252, -0.030300414189696312, 0.057151876389980316, 0.019868632778525352, 0.2748710811138153, 0.08226518332958221, 0.1255011409521103, 0.36939671635627747, 0.07104364782571793, 0.24104511737823486, -0.07618078589439392, 0.10349296778440475, -0.08450707048177719, 0.030008438974618912, 0.1794310361146927, 0.001266065868549049, 0.0, 0.10492678731679916, -0.037137772887945175, 0.24052436649799347]
    >,
    "kernel" => #Nx.Tensor<
      f32[8][20]
      EXLA.Backend<host:0, 0.3092930912.1915093000.103595>
      [
        [-0.26533785462379456, 0.36979803442955017, 0.2883588969707489, 0.05691013112664223, 0.1527024358510971, 0.21330364048480988, -0.16628126800060272, 0.227668896317482, 0.26195985078811646, 0.1335374265909195, 0.41601017117500305, -0.19829700887203217, -0.42029011249542236, 0.2193489670753479, -0.16464130580425262, 0.21972039341926575, -0.19151422381401062, 0.43836358189582825, 0.07292071729898453, 0.4228648543357849],
        [-0.10910817235708237, 0.3228389024734497, 0.32152920961380005, -0.22422561049461365, 0.5979372262954712, 0.3654206097126007, 0.26698580384254456, -0.18969188630580902, -0.36371639370918274, 0.16748982667922974, 0.06281175464391708, -0.20165325701236725, 0.05934212729334831, -0.040152568370103836, -0.050665054470300674, 0.44857028126716614, -0.19796931743621826, -0.3535391092300415, -0.29509082436561584, -0.2702084481716156],
        [0.011858348734676838, -0.2323773056268692, -0.02707824856042862, -0.19288890063762665, 0.36874818801879883, -0.040131907910108566, -0.20640146732330322, ...],
        ...
      ]
    >
  },
  "dense_1" => %{
    "bias" => #Nx.Tensor<
      f32[20]
      EXLA.Backend<host:0, 0.3092930912.1915093000.103596>
      [0.04999806731939316, 0.06232139840722084, 0.060841482132673264, 0.11127662658691406, -0.0655238926410675, 0.11876841634511948, 0.27493008971214294, 0.06880046427249908, -0.06998830288648605, 0.18649260699748993, 0.07336889952421188, 0.0, 0.11134245246648788, 0.16397754848003387, 0.0, -0.015887191519141197, 0.059302471578121185, 0.1369023621082306, 0.04787168279290199, 0.0539894625544548]
    >,
    "kernel" => #Nx.Tensor<
      f32[20][20]
      EXLA.Backend<host:0, 0.3092930912.1915093000.103597>
      [
        [0.3654071092605591, -0.4178696572780609, 0.3946247398853302, 0.40040966868400574, 0.1253684163093567, 0.3301914632320404, 0.2425689697265625, -0.04206422343850136, 0.1553564965724945, 0.28236985206604004, -0.5076530575752258, 0.20762550830841064, 0.3819800913333893, 0.2914133071899414, -0.16550993919372559, 0.2752481698989868, -0.45171719789505005, 0.2552894353866577, 0.2569112777709961, 0.2782590091228485],
        [0.22612954676151276, 0.18736112117767334, -0.013335414230823517, 0.07760892063379288, -0.06938071548938751, 0.38295525312423706, 0.11999505013227463, 0.32751667499542236, -0.07660731673240662, -0.10310948640108109, -0.11648740619421005, -0.32359015941619873, -0.22313036024570465, -0.2083093672990799, -0.30631738901138306, -0.4305667579174042, 0.14410987496376038, 0.050531577318906784, 0.24694404006004333, -0.23167836666107178],
        [-0.0630699023604393, 0.1519119292497635, 0.2548890709877014, -0.13860401511192322, -0.17398607730865479, 0.2190566211938858, ...],
        ...
      ]
    >
  },
  "dense_2" => %{
    "bias" => #Nx.Tensor<
      f32[2]
      EXLA.Backend<host:0, 0.3092930912.1915093000.103598>
      [-0.04483092203736305, 0.04483097046613693]
    >,
    "kernel" => #Nx.Tensor<
      f32[20][2]
      EXLA.Backend<host:0, 0.3092930912.1915093000.103599>
      [
        [0.06922014057636261, -0.3208019733428955],
        [-0.8277594447135925, 0.21217972040176392],
        [0.7316368222236633, -0.71186363697052],
        [0.30597633123397827, -0.5171008110046387],
        [0.2432984560728073, 0.29879170656204224],
        [0.37193799018859863, -0.2231573760509491],
        [0.5023778676986694, -0.4914725124835968],
        [-1.2406505346298218, 0.897369384765625],
        [0.3704480528831482, -0.38335368037223816],
        [0.5682506561279297, -0.5606864094734192],
        [-0.6137204766273499, 0.11450948566198349],
        [0.03636801242828369, 0.04292219877243042],
        [0.7472948431968689, -0.32217124104499817],
        [0.22907565534114838, -0.5166576504707336],
        [-0.3938654661178589, 0.5132765173912048],
        [0.378991037607193, -0.46209532022476196],
        [-0.042746126651763916, 0.44431018829345703],
        [-0.29302889108657837, 0.2976696491241455],
        [0.016075583174824715, 0.36041444540023804],
        [0.7297075986862183, -0.004295529332011938]
      ]
    >
  }
}
```

## Time to test

Let test were it failled before

```elixir
Axon.predict(
  model,
  trained0,
  %{
    "input_0" => Nx.tensor([[0, 0, 0, 1, 0, 0, 0, 0]])
  },
  compiler: EXLA
)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[1][2]
  EXLA.Backend<host:0, 0.3092930912.1915093000.103617>
  [
    [0.9417206048965454, 0.05827938765287399]
  ]
>
```



```elixir
Axon.predict(
  model,
  trained1,
  %{
    "input_0" => Nx.tensor([[0, 0, 0, 1, 0, 0, 0, 0]])
  },
  compiler: EXLA
)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[1][2]
  EXLA.Backend<host:0, 0.3092930912.1915093000.103619>
  [
    [0.9996722936630249, 3.2770048710517585e-4]
  ]
>
```

Much better, but too silly because on our trainning we gave this specific information, and probably more than one time

Would be more interesting if we ask for a *new* number, a number that we didn't gave on trainning

```elixir
raw_trainning_data_excluding = fn exclude ->
  raw_trainning_data
  |> Stream.filter(&(&1 != exclude))
end

raw_trainning_data_excluding.(23) |> Enum.take(10)
```

<!-- livebook:{"output":true} -->

```
[
  {241, false},
  {157, false},
  {105, false},
  {241, false},
  {183, false},
  {48, true},
  {45, false},
  {196, true},
  {103, false},
  {121, false}
]
```

```elixir
trained2 =
  model
  |> Axon.Loop.trainer(:categorical_cross_entropy, Axon.Optimizers.adamw(0.005))
  |> Axon.Loop.run(
    raw_trainning_data_excluding.(23)
    |> Stream.map(fn {n, is_even} ->
      {number2input_tensor.(n), even2output_tensor.(is_even)}
    end)
    |> Stream.take(1000),
    epochs: 1,
    compiler: EXLA
  )
```

<!-- livebook:{"output":true} -->

```
Epoch: 0, Batch: 950, loss: 0.0580244
```

<!-- livebook:{"output":true} -->

```
%{
  "dense_0" => %{
    "bias" => #Nx.Tensor<
      f32[20]
      EXLA.Backend<host:0, 0.3092930912.1915093000.129644>
      [0.05281147360801697, -0.05722160264849663, -0.10457045584917068, 0.22393988072872162, -0.09610438346862793, 0.019765375182032585, 0.26378774642944336, 0.09070166200399399, 0.4480665922164917, 0.2982180416584015, 0.003931756131350994, 0.07448625564575195, 0.02237958461046219, 0.02063935436308384, -0.14472705125808716, -0.07269375771284103, 0.10459303855895996, 0.27918681502342224, -0.06347884237766266, 0.11322083324193954]
    >,
    "kernel" => #Nx.Tensor<
      f32[8][20]
      EXLA.Backend<host:0, 0.3092930912.1915093000.129645>
      [
        [-0.020377757027745247, 0.19303017854690552, 0.2449706643819809, -0.11913974583148956, -0.0574604868888855, -0.005896157585084438, 0.34887781739234924, -0.058726705610752106, 0.008717353455722332, 0.3815000653266907, 0.41478222608566284, -0.06431746482849121, 0.06884431093931198, 0.10692525655031204, 0.06634983420372009, -0.05639858916401863, -0.20326358079910278, -0.06419117748737335, -0.3018019497394562, 0.39715564250946045],
        [-0.05420518293976784, -0.41013970971107483, -0.42259910702705383, 0.32633835077285767, 0.11684058606624603, 0.15493226051330566, 0.23591317236423492, 0.11475466936826706, 0.26335403323173523, 0.02691725455224514, 0.4179699718952179, -0.39955610036849976, 0.450330525636673, -0.34748682379722595, -0.42096078395843506, -0.4043128490447998, 0.008378900587558746, -0.32937583327293396, -0.3662472069263458, 0.20116056501865387],
        [0.1805032640695572, -0.05291371047496796, 0.09877606481313705, 0.386438250541687, -0.21825523674488068, 0.29630246758461, 0.07274462282657623, ...],
        ...
      ]
    >
  },
  "dense_1" => %{
    "bias" => #Nx.Tensor<
      f32[20]
      EXLA.Backend<host:0, 0.3092930912.1915093000.129646>
      [0.18582521378993988, -0.04076983779668808, 0.01626857928931713, 0.0024707855191081762, -0.041472189128398895, -0.03088034875690937, 0.0916370302438736, 0.13891847431659698, -0.011461450718343258, -0.022541815415024757, 0.05625056475400925, 0.09340228885412216, 0.12917082011699677, 0.08879116922616959, -0.07128950208425522, -0.054151397198438644, 0.17987006902694702, -0.08743361383676529, -0.023833902552723885, 0.2301882803440094]
    >,
    "kernel" => #Nx.Tensor<
      f32[20][20]
      EXLA.Backend<host:0, 0.3092930912.1915093000.129647>
      [
        [0.0019462179625406861, -0.2890292704105377, 0.37916815280914307, -0.1872473657131195, -0.12077010422945023, -0.25824204087257385, 0.409071147441864, -0.27204856276512146, 0.7115374803543091, 0.24238036572933197, -0.0020666345953941345, 0.004230875987559557, -0.20439329743385315, -0.09377707540988922, 0.7374395728111267, 0.27967002987861633, -0.24141822755336761, 0.3717348277568817, -0.10315009206533432, -0.030239304527640343],
        [0.09619849920272827, 0.021085649728775024, -0.1758827567100525, 0.3110843896865845, -0.04831794276833534, 0.2826085090637207, 0.11438783258199692, -0.26057058572769165, 0.27197474241256714, 0.011003486812114716, 0.098919577896595, -0.29121556878089905, -0.1776617467403412, -0.014659089967608452, -0.43968525528907776, 0.29827937483787537, 0.2631342113018036, 0.16187262535095215, -0.3402753174304962, 0.29506441950798035],
        [0.12087466567754745, -0.13563072681427002, 0.22205650806427002, 0.4408673048019409, -0.32205116748809814, -0.2426774650812149, ...],
        ...
      ]
    >
  },
  "dense_2" => %{
    "bias" => #Nx.Tensor<
      f32[2]
      EXLA.Backend<host:0, 0.3092930912.1915093000.129648>
      [0.0608987919986248, -0.06089870631694794]
    >,
    "kernel" => #Nx.Tensor<
      f32[20][2]
      EXLA.Backend<host:0, 0.3092930912.1915093000.129649>
      [
        [0.7345006465911865, -0.38611897826194763],
        [0.02648545801639557, -9.166975505650043e-4],
        [-0.5571007132530212, 0.14406323432922363],
        [0.3598029315471649, -0.038305122405290604],
        [0.19320181012153625, 0.29862016439437866],
        [0.4137437641620636, -0.47082677483558655],
        [-0.45161548256874084, 0.6867528557777405],
        [0.7223514914512634, -0.1709226816892624],
        [-0.5783615708351135, 0.25271955132484436],
        [-0.5974807739257812, 0.03954363241791725],
        [0.5805943012237549, 0.11008241772651672],
        [0.02302257902920246, -0.494803786277771],
        [-0.3584288954734802, -0.6493043899536133],
        [0.6195384860038757, 0.11179636418819427],
        [0.09010865539312363, 0.531109631061554],
        [-0.6056551933288574, 0.06256213784217834],
        [0.16475071012973785, -0.5776251554489136],
        [0.08772463351488113, 0.21582990884780884],
        [0.08511528372764587, -0.40146955847740173],
        [0.5956469774246216, -0.9297441244125366]
      ]
    >
  }
}
```

## Finally...

And let guest to our NN the oddity of the excluded number

```elixir
Axon.predict(
  model,
  trained2,
  %{
    "input_0" => number2input_tensor.(23)
  },
  compiler: EXLA
)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[1][2]
  EXLA.Backend<host:0, 0.3092930912.1915093000.129671>
  [
    [4.192412961856462e-5, 0.9999580383300781]
  ]
>
```

Our NN is very, very, very, very sure that a number it doesn't know is odd, **and it's right**
